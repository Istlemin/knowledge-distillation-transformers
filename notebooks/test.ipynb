{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Queue\n",
    "import multiprocessing\n",
    "from pathos.multiprocessing import Pool\n",
    "from datasets.arrow_dataset import Dataset\n",
    "from transformers import PreTrainedTokenizer\n",
    "from typing import List, Dict\n",
    "import torch\n",
    "import random\n",
    "import tqdm\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from datasets import concatenate_datasets\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "mlm_dataset_path = Path(\"../../wikipedia_mlm/\")\n",
    "\n",
    "datasets = [\n",
    "    Dataset.load_from_disk(path)\n",
    "    for path in sorted(glob.glob(str(mlm_dataset_path / \"*\")))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 162948\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 172999\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 160956\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 175229\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 162693\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 168274\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 167903\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 161565\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 162859\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 165703\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 169797\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 166866\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 155920\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 564459\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 462027\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 366900\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 312681\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 293901\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 274809\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 252192\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 244789\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 231538\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 224537\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 159805\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 154380\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 154070\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 154646\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 157720\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 152376\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 148781\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 154994\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 154683\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 145809\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 156688\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 154675\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 215324\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 203911\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 200715\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 202386\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 197290\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 196395\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 185717\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 164728\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 180381\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 171809\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 159278\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 163572\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 162664\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 157510\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 174769\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 172961\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 162658\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 163493\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 160988\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 151511\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 158048\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 161696\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 156221\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 161447\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['tokens', 'masked_tokens', 'is_masked'],\n",
       "     num_rows: 159244\n",
       " })]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['              total        used        free      shared  buff/cache   available\\n',\n",
       " 'Mem:         128800       41259       33827           5       53713       86274\\n',\n",
       " 'Swap:          1766        1520         245\\n',\n",
       " 'Total:       130566       42780       34072\\n']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\"\\n\".join(os.popen('free -t -m').readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1689\n"
     ]
    }
   ],
   "source": [
    "import os, psutil\n",
    "process = psutil.Process(os.getpid())\n",
    "print(process.memory_info().rss//1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transform <function _concatenate_map_style_datasets at 0x7f33d57a59d0> couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [
    "dataset = concatenate_datasets(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:11<00:00, 84.48it/s]\n"
     ]
    }
   ],
   "source": [
    "x = 0\n",
    "for i in tqdm.tqdm(range(1000)):\n",
    "    x += len(dataset[random.randint(0,len(dataset)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab29bdd578247210423fe399e48aecd0ef8b6f86b36785b7520d6025e6beafbf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
